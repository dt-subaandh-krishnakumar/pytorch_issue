Torchserve version: 0.8.1
TS Home: /home/venv/lib/python3.9/site-packages
Current directory: /home/model-server
Temp directory: /home/model-server/tmp
Metrics config path: /home/venv/lib/python3.9/site-packages/ts/configs/metrics.yaml
Number of GPUs: 1
Number of CPUs: 4
Max heap size: 5708 M
Python executable: /home/venv/bin/python
Config file: /home/model-server/shared/LanguageModel/config/test.properties
Inference address: http://0.0.0.0:8080
Management address: http://0.0.0.0:8081
Metrics address: http://0.0.0.0:8082
Model Store: /home/model-server/shared/LanguageModel/model-store
Initial Models: distilbert=distilbert_model.mar
Log dir: /home/model-server/logs
Metrics dir: /home/model-server/logs
Netty threads: 32
Netty client threads: 0
Default workers per model: 1
Blacklist Regex: N/A
Maximum Response Size: 128000000
Maximum Request Size: 128000000
Limit Maximum Image Pixels: true
Prefer direct buffer: false
Allowed Urls: [file://.*|http(s)?://.*]
Custom python dependency for model allowed: false
Enable metrics API: true
Metrics mode: log
Disable system metrics: false
Workflow Store: /home/model-server/shared/LanguageModel/model-store
Model config: N/A
2023-06-26T14:38:13,241 [INFO ] main org.pytorch.serve.ModelServer - 
Torchserve version: 0.8.1
TS Home: /home/venv/lib/python3.9/site-packages
Current directory: /home/model-server
Temp directory: /home/model-server/tmp
Metrics config path: /home/venv/lib/python3.9/site-packages/ts/configs/metrics.yaml
Number of GPUs: 1
Number of CPUs: 4
Max heap size: 5708 M
Python executable: /home/venv/bin/python
Config file: /home/model-server/shared/LanguageModel/config/test.properties
Inference address: http://0.0.0.0:8080
Management address: http://0.0.0.0:8081
Metrics address: http://0.0.0.0:8082
Model Store: /home/model-server/shared/LanguageModel/model-store
Initial Models: distilbert=distilbert_model.mar
Log dir: /home/model-server/logs
Metrics dir: /home/model-server/logs
Netty threads: 32
Netty client threads: 0
Default workers per model: 1
Blacklist Regex: N/A
Maximum Response Size: 128000000
Maximum Request Size: 128000000
Limit Maximum Image Pixels: true
Prefer direct buffer: false
Allowed Urls: [file://.*|http(s)?://.*]
Custom python dependency for model allowed: false
Enable metrics API: true
Metrics mode: log
Disable system metrics: false
Workflow Store: /home/model-server/shared/LanguageModel/model-store
Model config: N/A
2023-06-26T14:38:13,248 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager -  Loading snapshot serializer plugin...
2023-06-26T14:38:13,248 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager -  Loading snapshot serializer plugin...
2023-06-26T14:38:13,267 [INFO ] main org.pytorch.serve.ModelServer - Loading initial models: distilbert_model.mar
2023-06-26T14:38:13,267 [INFO ] main org.pytorch.serve.ModelServer - Loading initial models: distilbert_model.mar
2023-06-26T14:38:18,087 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Adding new version 1 for model distilbert
2023-06-26T14:38:18,087 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Adding new version 1 for model distilbert
2023-06-26T14:38:18,087 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1 for model distilbert
2023-06-26T14:38:18,087 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1 for model distilbert
2023-06-26T14:38:18,087 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model distilbert loaded.
2023-06-26T14:38:18,087 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model distilbert loaded.
2023-06-26T14:38:18,088 [DEBUG] main org.pytorch.serve.wlm.ModelManager - updateModel: distilbert, count: 1
2023-06-26T14:38:18,088 [DEBUG] main org.pytorch.serve.wlm.ModelManager - updateModel: distilbert, count: 1
2023-06-26T14:38:18,096 [DEBUG] W-9000-distilbert_1 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/home/venv/bin/python, /home/venv/lib/python3.9/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /home/model-server/tmp/.ts.sock.9000, --metrics-config, /home/venv/lib/python3.9/site-packages/ts/configs/metrics.yaml]
2023-06-26T14:38:18,096 [DEBUG] W-9000-distilbert_1 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/home/venv/bin/python, /home/venv/lib/python3.9/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /home/model-server/tmp/.ts.sock.9000, --metrics-config, /home/venv/lib/python3.9/site-packages/ts/configs/metrics.yaml]
2023-06-26T14:38:18,097 [INFO ] main org.pytorch.serve.ModelServer - Initialize Inference server with: EpollServerSocketChannel.
2023-06-26T14:38:18,097 [INFO ] main org.pytorch.serve.ModelServer - Initialize Inference server with: EpollServerSocketChannel.
2023-06-26T14:38:18,161 [INFO ] main org.pytorch.serve.ModelServer - Inference API bind to: http://0.0.0.0:8080
2023-06-26T14:38:18,161 [INFO ] main org.pytorch.serve.ModelServer - Inference API bind to: http://0.0.0.0:8080
2023-06-26T14:38:18,161 [INFO ] main org.pytorch.serve.ModelServer - Initialize Management server with: EpollServerSocketChannel.
2023-06-26T14:38:18,161 [INFO ] main org.pytorch.serve.ModelServer - Initialize Management server with: EpollServerSocketChannel.
2023-06-26T14:38:18,166 [INFO ] main org.pytorch.serve.ModelServer - Management API bind to: http://0.0.0.0:8081
2023-06-26T14:38:18,166 [INFO ] main org.pytorch.serve.ModelServer - Management API bind to: http://0.0.0.0:8081
2023-06-26T14:38:18,166 [INFO ] main org.pytorch.serve.ModelServer - Initialize Metrics server with: EpollServerSocketChannel.
2023-06-26T14:38:18,166 [INFO ] main org.pytorch.serve.ModelServer - Initialize Metrics server with: EpollServerSocketChannel.
2023-06-26T14:38:18,167 [INFO ] main org.pytorch.serve.ModelServer - Metrics API bind to: http://0.0.0.0:8082
2023-06-26T14:38:18,167 [INFO ] main org.pytorch.serve.ModelServer - Metrics API bind to: http://0.0.0.0:8082
2023-06-26T14:38:18,443 [WARN ] pool-3-thread-1 org.pytorch.serve.metrics.MetricCollector - worker pid is not available yet.
2023-06-26T14:38:18,443 [WARN ] pool-3-thread-1 org.pytorch.serve.metrics.MetricCollector - worker pid is not available yet.
2023-06-26T14:38:18,944 [INFO ] pool-3-thread-1 TS_METRICS - CPUUtilization.Percent:66.7|#Level:Host|#hostname:torchserve-service-lm-68874fb5-9vl6t,timestamp:1687790298
2023-06-26T14:38:18,947 [INFO ] pool-3-thread-1 TS_METRICS - DiskAvailable.Gigabytes:72.80280685424805|#Level:Host|#hostname:torchserve-service-lm-68874fb5-9vl6t,timestamp:1687790298
2023-06-26T14:38:18,948 [INFO ] pool-3-thread-1 TS_METRICS - DiskUsage.Gigabytes:21.435020446777344|#Level:Host|#hostname:torchserve-service-lm-68874fb5-9vl6t,timestamp:1687790298
2023-06-26T14:38:18,950 [INFO ] pool-3-thread-1 TS_METRICS - DiskUtilization.Percent:22.7|#Level:Host|#hostname:torchserve-service-lm-68874fb5-9vl6t,timestamp:1687790298
2023-06-26T14:38:18,952 [INFO ] pool-3-thread-1 TS_METRICS - GPUMemoryUtilization.Percent:0.0|#Level:Host,DeviceId:0|#hostname:torchserve-service-lm-68874fb5-9vl6t,timestamp:1687790298
2023-06-26T14:38:18,953 [INFO ] pool-3-thread-1 TS_METRICS - GPUMemoryUsed.Megabytes:0.0|#Level:Host,DeviceId:0|#hostname:torchserve-service-lm-68874fb5-9vl6t,timestamp:1687790298
2023-06-26T14:38:18,955 [INFO ] pool-3-thread-1 TS_METRICS - GPUUtilization.Percent:0.0|#Level:Host,DeviceId:0|#hostname:torchserve-service-lm-68874fb5-9vl6t,timestamp:1687790298
2023-06-26T14:38:18,955 [INFO ] pool-3-thread-1 TS_METRICS - MemoryAvailable.Megabytes:24674.56640625|#Level:Host|#hostname:torchserve-service-lm-68874fb5-9vl6t,timestamp:1687790298
2023-06-26T14:38:18,956 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUsed.Megabytes:945.77734375|#Level:Host|#hostname:torchserve-service-lm-68874fb5-9vl6t,timestamp:1687790298
2023-06-26T14:38:18,958 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUtilization.Percent:5.3|#Level:Host|#hostname:torchserve-service-lm-68874fb5-9vl6t,timestamp:1687790298
2023-06-26T14:38:19,833 [INFO ] W-9000-distilbert_1-stdout MODEL_LOG - s_name_part0=/home/model-server/tmp/.ts.sock, s_name_part1=9000, pid=158
2023-06-26T14:38:19,834 [INFO ] W-9000-distilbert_1-stdout MODEL_LOG - Listening on port: /home/model-server/tmp/.ts.sock.9000
2023-06-26T14:38:19,844 [INFO ] W-9000-distilbert_1-stdout MODEL_LOG - Successfully loaded /home/venv/lib/python3.9/site-packages/ts/configs/metrics.yaml.
2023-06-26T14:38:19,845 [INFO ] W-9000-distilbert_1-stdout MODEL_LOG - [PID]158
2023-06-26T14:38:19,845 [INFO ] W-9000-distilbert_1-stdout MODEL_LOG - Torch worker started.
2023-06-26T14:38:19,845 [INFO ] W-9000-distilbert_1-stdout MODEL_LOG - Python runtime: 3.9.17
2023-06-26T14:38:19,846 [DEBUG] W-9000-distilbert_1 org.pytorch.serve.wlm.WorkerThread - W-9000-distilbert_1 State change null -> WORKER_STARTED
2023-06-26T14:38:19,846 [DEBUG] W-9000-distilbert_1 org.pytorch.serve.wlm.WorkerThread - W-9000-distilbert_1 State change null -> WORKER_STARTED
2023-06-26T14:38:19,851 [INFO ] W-9000-distilbert_1 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9000
2023-06-26T14:38:19,851 [INFO ] W-9000-distilbert_1 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9000
2023-06-26T14:38:19,860 [INFO ] W-9000-distilbert_1-stdout MODEL_LOG - Connection accepted: /home/model-server/tmp/.ts.sock.9000.
2023-06-26T14:38:19,863 [INFO ] W-9000-distilbert_1 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD to backend at: 1687790299863
2023-06-26T14:38:19,863 [INFO ] W-9000-distilbert_1 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD to backend at: 1687790299863
2023-06-26T14:38:19,911 [INFO ] W-9000-distilbert_1-stdout MODEL_LOG - model_name: distilbert, batchSize: 1
2023-06-26T14:38:20,274 [INFO ] W-9000-distilbert_1-stdout MODEL_LOG - ONNX enabled
2023-06-26T14:38:20,275 [INFO ] W-9000-distilbert_1-stdout MODEL_LOG - {'model_dir': '/home/model-server/tmp/models/f770b00ce57c471ba98aaeede3ec4fd0', 'gpu_id': 0, 'batch_size': 1, 'server_name': 'MMS', 'server_version': '0.8.1', 'limit_max_image_pixels': True}
2023-06-26T14:38:20,685 [WARN ] W-9000-distilbert_1-stderr MODEL_LOG - 
2023-06-26T14:38:20,694 [WARN ] W-9000-distilbert_1-stderr MODEL_LOG - Downloading (…)okenizer_config.json:   0%|          | 0.00/28.0 [00:00<?, ?B/s]
2023-06-26T14:38:20,694 [WARN ] W-9000-distilbert_1-stderr MODEL_LOG - Downloading (…)okenizer_config.json: 100%|██████████| 28.0/28.0 [00:00<00:00, 3.30kB/s]
2023-06-26T14:38:20,896 [WARN ] W-9000-distilbert_1-stderr MODEL_LOG - 
2023-06-26T14:38:20,903 [WARN ] W-9000-distilbert_1-stderr MODEL_LOG - Downloading (…)lve/main/config.json:   0%|          | 0.00/483 [00:00<?, ?B/s]
2023-06-26T14:38:20,904 [WARN ] W-9000-distilbert_1-stderr MODEL_LOG - Downloading (…)lve/main/config.json: 100%|██████████| 483/483 [00:00<00:00, 65.7kB/s]
2023-06-26T14:38:21,112 [WARN ] W-9000-distilbert_1-stderr MODEL_LOG - 
2023-06-26T14:38:21,280 [WARN ] W-9000-distilbert_1-stderr MODEL_LOG - Downloading (…)solve/main/vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]
2023-06-26T14:38:21,281 [WARN ] W-9000-distilbert_1-stderr MODEL_LOG - Downloading (…)solve/main/vocab.txt: 100%|██████████| 232k/232k [00:00<00:00, 1.38MB/s]
2023-06-26T14:38:21,281 [WARN ] W-9000-distilbert_1-stderr MODEL_LOG - Downloading (…)solve/main/vocab.txt: 100%|██████████| 232k/232k [00:00<00:00, 1.38MB/s]
2023-06-26T14:38:21,506 [WARN ] W-9000-distilbert_1-stderr MODEL_LOG - 
2023-06-26T14:38:21,752 [WARN ] W-9000-distilbert_1-stderr MODEL_LOG - Downloading (…)/main/tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]
2023-06-26T14:38:21,753 [WARN ] W-9000-distilbert_1-stderr MODEL_LOG - Downloading (…)/main/tokenizer.json: 100%|██████████| 466k/466k [00:00<00:00, 1.89MB/s]
2023-06-26T14:38:21,754 [WARN ] W-9000-distilbert_1-stderr MODEL_LOG - Downloading (…)/main/tokenizer.json: 100%|██████████| 466k/466k [00:00<00:00, 1.89MB/s]
2023-06-26T14:38:22,261 [WARN ] W-9000-distilbert_1-stderr MODEL_LOG - 2023-06-26 14:38:22.261212984 [W:onnxruntime:Default, onnxruntime_pybind_state.cc:640 CreateExecutionProviderInstance] Failed to create CUDAExecutionProvider. Please reference https://onnxruntime.ai/docs/execution-providers/CUDA-ExecutionProvider.html#requirements to ensure all dependencies are met.

2023-06-26T14:38:22,605 [WARN ] W-9000-distilbert_1-stderr MODEL_LOG - 2023-06-26 14:38:22.605120896 [W:onnxruntime:, inference_session.cc:1591 Initialize] Serializing optimized model with Graph Optimization level greater than ORT_ENABLE_EXTENDED and the NchwcTransformer enabled. The generated model may contain hardware specific optimizations, and should only be used in the same environment the model was optimized in.
2023-06-26T14:38:24,625 [DEBUG] W-9000-distilbert_1 org.pytorch.serve.wlm.WorkerThread - sent a reply, jobdone: true
2023-06-26T14:38:24,625 [DEBUG] W-9000-distilbert_1 org.pytorch.serve.wlm.WorkerThread - sent a reply, jobdone: true
2023-06-26T14:38:24,625 [INFO ] W-9000-distilbert_1 org.pytorch.serve.wlm.WorkerThread - Backend response time: 4712
2023-06-26T14:38:24,625 [INFO ] W-9000-distilbert_1 org.pytorch.serve.wlm.WorkerThread - Backend response time: 4712
2023-06-26T14:38:24,626 [DEBUG] W-9000-distilbert_1 org.pytorch.serve.wlm.WorkerThread - W-9000-distilbert_1 State change WORKER_STARTED -> WORKER_MODEL_LOADED
2023-06-26T14:38:24,626 [DEBUG] W-9000-distilbert_1 org.pytorch.serve.wlm.WorkerThread - W-9000-distilbert_1 State change WORKER_STARTED -> WORKER_MODEL_LOADED
2023-06-26T14:38:24,626 [INFO ] W-9000-distilbert_1 TS_METRICS - WorkerLoadTime.Milliseconds:6533.0|#WorkerName:W-9000-distilbert_1,Level:Host|#hostname:torchserve-service-lm-68874fb5-9vl6t,timestamp:1687790304
2023-06-26T14:38:24,627 [INFO ] W-9000-distilbert_1 TS_METRICS - WorkerThreadTime.Milliseconds:52.0|#Level:Host|#hostname:torchserve-service-lm-68874fb5-9vl6t,timestamp:1687790304
2023-06-26T14:38:33,840 [INFO ] pool-2-thread-2 ACCESS_LOG - /10.100.0.117:45300 "GET /ping HTTP/1.1" 200 7
2023-06-26T14:38:33,841 [INFO ] pool-2-thread-2 TS_METRICS - Requests2XX.Count:1.0|#Level:Host|#hostname:torchserve-service-lm-68874fb5-9vl6t,timestamp:1687790313
2023-06-26T14:39:01,286 [INFO ] epollEventLoopGroup-3-2 ACCESS_LOG - /127.0.0.1:48494 "GET /models HTTP/1.1" 200 2
2023-06-26T14:39:01,287 [INFO ] epollEventLoopGroup-3-2 TS_METRICS - Requests2XX.Count:1.0|#Level:Host|#hostname:torchserve-service-lm-68874fb5-9vl6t,timestamp:1687790341
2023-06-26T14:39:05,894 [INFO ] epollEventLoopGroup-3-3 ACCESS_LOG - /127.0.0.1:48508 "GET /models/distilbert HTTP/1.1" 200 15
2023-06-26T14:39:05,895 [INFO ] epollEventLoopGroup-3-3 TS_METRICS - Requests2XX.Count:1.0|#Level:Host|#hostname:torchserve-service-lm-68874fb5-9vl6t,timestamp:1687790345
model-server@torchserve-service-lm-68874fb5-9vl6t:~$ 2023-06-26T14:39:18,913 [INFO ] pool-3-thread-1 TS_METRICS - CPUUtilization.Percent:0.0|#Level:Host|#hostname:torchserve-service-lm-68874fb5-9vl6t,timestamp:1687790358
2023-06-26T14:39:18,914 [INFO ] pool-3-thread-1 TS_METRICS - DiskAvailable.Gigabytes:72.80198669433594|#Level:Host|#hostname:torchserve-service-lm-68874fb5-9vl6t,timestamp:1687790358
2023-06-26T14:39:18,914 [INFO ] pool-3-thread-1 TS_METRICS - DiskUsage.Gigabytes:21.435840606689453|#Level:Host|#hostname:torchserve-service-lm-68874fb5-9vl6t,timestamp:1687790358
2023-06-26T14:39:18,915 [INFO ] pool-3-thread-1 TS_METRICS - DiskUtilization.Percent:22.7|#Level:Host|#hostname:torchserve-service-lm-68874fb5-9vl6t,timestamp:1687790358
2023-06-26T14:39:18,916 [INFO ] pool-3-thread-1 TS_METRICS - GPUMemoryUtilization.Percent:0.019855715136673505|#Level:Host,DeviceId:0|#hostname:torchserve-service-lm-68874fb5-9vl6t,timestamp:1687790358
2023-06-26T14:39:18,916 [INFO ] pool-3-thread-1 TS_METRICS - GPUMemoryUsed.Megabytes:3.0|#Level:Host,DeviceId:0|#hostname:torchserve-service-lm-68874fb5-9vl6t,timestamp:1687790358
2023-06-26T14:39:18,917 [INFO ] pool-3-thread-1 TS_METRICS - GPUUtilization.Percent:0.0|#Level:Host,DeviceId:0|#hostname:torchserve-service-lm-68874fb5-9vl6t,timestamp:1687790358
2023-06-26T14:39:18,917 [INFO ] pool-3-thread-1 TS_METRICS - MemoryAvailable.Megabytes:23771.96484375|#Level:Host|#hostname:torchserve-service-lm-68874fb5-9vl6t,timestamp:1687790358
2023-06-26T14:39:18,918 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUsed.Megabytes:1848.375|#Level:Host|#hostname:torchserve-service-lm-68874fb5-9vl6t,timestamp:1687790358
2023-06-26T14:39:18,918 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUtilization.Percent:8.8|#Level:Host|#hostname:torchserve-service-lm-68874fb5-9vl6t,timestamp:1687790358
^C
model-server@torchserve-service-lm-68874fb5-9vl6t:~$ nvidia-smi 
Mon Jun 26 14:39:26 2023       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 470.161.03   Driver Version: 470.161.03   CUDA Version: 11.4     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |
| N/A   37C    P8     9W /  70W |      3MiB / 15109MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                                  |
|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
|        ID   ID                                                   Usage      |
|=============================================================================|
|  No running processes found                                                 |
+-----------------------------------------------------------------------------+
model-server@torchserve-service-lm-68874fb5-9vl6t:~$ pip list2023-06-26T14:39:30,285 [INFO ] epollEventLoopGroup-4-1 ACCESS_LOG - /10.92.5.8:59384 "GET /metrics HTTP/1.1" 200 2
2023-06-26T14:39:30,286 [INFO ] epollEventLoopGroup-4-1 TS_METRICS - Requests2XX.Count:1.0|#Level:Host|#hostname:torchserve-service-lm-68874fb5-9vl6t,timestamp:1687790370

Package                 Version
----------------------- ------------
aniso8601               9.0.1
ansi2html               1.8.0
arrow                   1.2.3
blinker                 1.6.2
captum                  0.6.0
certifi                 2023.5.7
charset-normalizer      3.1.0
click                   8.1.3
cmake                   3.26.4
coloredlogs             15.0.1
contourpy               1.1.0
cycler                  0.11.0
Cython                  0.29.34
enum-compat             0.0.3
filelock                3.12.2
Flask                   2.3.2
Flask-RESTful           0.3.10
flatbuffers             23.5.26
fonttools               4.40.0
fsspec                  2023.6.0
huggingface-hub         0.15.1
humanfriendly           10.0
idna                    3.4
importlib-metadata      6.6.0
importlib-resources     5.12.0
itsdangerous            2.1.2
Jinja2                  3.1.2
kiwisolver              1.4.4
lit                     16.0.5.post0
MarkupSafe              2.1.3
matplotlib              3.7.1
mpmath                  1.3.0
networkx                3.1
numpy                   1.24.3
nvgpu                   0.10.0
onnx                    1.14.0
onnxruntime-gpu         1.15.1
packaging               23.1
pandas                  2.0.2
Pillow                  9.3.0
pip                     23.1.2
protobuf                4.23.3
psutil                  5.9.5
pynvml                  11.4.1
pyparsing               3.0.9
python-dateutil         2.8.2
pytz                    2023.3
PyYAML                  6.0
regex                   2023.6.3
requests                2.31.0
safetensors             0.3.1
setuptools              67.8.0
six                     1.16.0
sympy                   1.12
tabulate                0.9.0
termcolor               2.3.0
tokenizers              0.13.3
torch                   2.0.1+cu117
torch-model-archiver    0.8.1
torch-workflow-archiver 0.2.9
torchaudio              2.0.2+cu117
torchdata               0.6.1
torchserve              0.8.1
torchtext               0.15.2+cpu
torchvision             0.15.2+cu117
tqdm                    4.65.0
transformers            4.30.2
triton                  2.0.0
typing_extensions       4.6.3
tzdata                  2023.3
urllib3                 2.0.3
Werkzeug                2.3.6
wheel                   0.40.0
zipp                    3.15.0
model-server@torchserve-service-lm-68874fb5-9vl6t:~$ 

[
  {
    "modelName": "distilbert",
    "modelVersion": "1",
    "modelUrl": "distilbert_model.mar",
    "runtime": "python",
    "minWorkers": 1,
    "maxWorkers": 1,
    "batchSize": 1,
    "maxBatchDelay": 100,
    "loadedAtStartup": true,
    "workers": [
      {
        "id": "9000",
        "startTime": "2023-06-26T14:38:18.093Z",
        "status": "READY",
        "memoryUsage": 0,
        "pid": 158,
        "gpu": true,
        "gpuUsage": "gpuId::0 utilization.gpu [%]::0 % utilization.memory [%]::0 % memory.used [MiB]::3 MiB"
      }
    ]
  }
]

