Torchserve version: 0.8.0
TS Home: /home/venv/lib/python3.9/site-packages
Current directory: /home/model-server
Temp directory: /home/model-server/tmp
Metrics config path: /home/venv/lib/python3.9/site-packages/ts/configs/metrics.yaml
Number of GPUs: 1
Number of CPUs: 4
Max heap size: 5708 M
Python executable: /home/venv/bin/python
Config file: /home/model-server/shared/LanguageModel/config/test.properties
Inference address: http://0.0.0.0:8080
Management address: http://0.0.0.0:8081
Metrics address: http://0.0.0.0:8082
Model Store: /home/model-server/shared/LanguageModel/model-store
Initial Models: distilbert=distilbert_model.mar
Log dir: /home/model-server/logs
Metrics dir: /home/model-server/logs
Netty threads: 32
Netty client threads: 0
Default workers per model: 1
Blacklist Regex: N/A
Maximum Response Size: 128000000
Maximum Request Size: 128000000
Limit Maximum Image Pixels: true
Prefer direct buffer: false
Allowed Urls: [file://.*|http(s)?://.*]
Custom python dependency for model allowed: false
Enable metrics API: true
Metrics mode: log
Disable system metrics: false
Workflow Store: /home/model-server/shared/LanguageModel/model-store
Model config: N/A
2023-06-26T15:04:41,983 [INFO ] main org.pytorch.serve.ModelServer - 
Torchserve version: 0.8.0
TS Home: /home/venv/lib/python3.9/site-packages
Current directory: /home/model-server
Temp directory: /home/model-server/tmp
Metrics config path: /home/venv/lib/python3.9/site-packages/ts/configs/metrics.yaml
Number of GPUs: 1
Number of CPUs: 4
Max heap size: 5708 M
Python executable: /home/venv/bin/python
Config file: /home/model-server/shared/LanguageModel/config/test.properties
Inference address: http://0.0.0.0:8080
Management address: http://0.0.0.0:8081
Metrics address: http://0.0.0.0:8082
Model Store: /home/model-server/shared/LanguageModel/model-store
Initial Models: distilbert=distilbert_model.mar
Log dir: /home/model-server/logs
Metrics dir: /home/model-server/logs
Netty threads: 32
Netty client threads: 0
Default workers per model: 1
Blacklist Regex: N/A
Maximum Response Size: 128000000
Maximum Request Size: 128000000
Limit Maximum Image Pixels: true
Prefer direct buffer: false
Allowed Urls: [file://.*|http(s)?://.*]
Custom python dependency for model allowed: false
Enable metrics API: true
Metrics mode: log
Disable system metrics: false
Workflow Store: /home/model-server/shared/LanguageModel/model-store
Model config: N/A
2023-06-26T15:04:41,994 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager -  Loading snapshot serializer plugin...
2023-06-26T15:04:41,994 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager -  Loading snapshot serializer plugin...
2023-06-26T15:04:42,024 [INFO ] main org.pytorch.serve.ModelServer - Loading initial models: distilbert_model.mar
2023-06-26T15:04:42,024 [INFO ] main org.pytorch.serve.ModelServer - Loading initial models: distilbert_model.mar
2023-06-26T15:04:48,715 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Adding new version 1 for model distilbert
2023-06-26T15:04:48,715 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Adding new version 1 for model distilbert
2023-06-26T15:04:48,715 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1 for model distilbert
2023-06-26T15:04:48,715 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1 for model distilbert
2023-06-26T15:04:48,715 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model distilbert loaded.
2023-06-26T15:04:48,715 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model distilbert loaded.
2023-06-26T15:04:48,716 [DEBUG] main org.pytorch.serve.wlm.ModelManager - updateModel: distilbert, count: 1
2023-06-26T15:04:48,716 [DEBUG] main org.pytorch.serve.wlm.ModelManager - updateModel: distilbert, count: 1
2023-06-26T15:04:48,725 [DEBUG] W-9000-distilbert_1 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/home/venv/bin/python, /home/venv/lib/python3.9/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /home/model-server/tmp/.ts.sock.9000, --metrics-config, /home/venv/lib/python3.9/site-packages/ts/configs/metrics.yaml]
2023-06-26T15:04:48,725 [DEBUG] W-9000-distilbert_1 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/home/venv/bin/python, /home/venv/lib/python3.9/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /home/model-server/tmp/.ts.sock.9000, --metrics-config, /home/venv/lib/python3.9/site-packages/ts/configs/metrics.yaml]
2023-06-26T15:04:48,727 [INFO ] main org.pytorch.serve.ModelServer - Initialize Inference server with: EpollServerSocketChannel.
2023-06-26T15:04:48,727 [INFO ] main org.pytorch.serve.ModelServer - Initialize Inference server with: EpollServerSocketChannel.
2023-06-26T15:04:48,796 [INFO ] main org.pytorch.serve.ModelServer - Inference API bind to: http://0.0.0.0:8080
2023-06-26T15:04:48,796 [INFO ] main org.pytorch.serve.ModelServer - Inference API bind to: http://0.0.0.0:8080
2023-06-26T15:04:48,796 [INFO ] main org.pytorch.serve.ModelServer - Initialize Management server with: EpollServerSocketChannel.
2023-06-26T15:04:48,796 [INFO ] main org.pytorch.serve.ModelServer - Initialize Management server with: EpollServerSocketChannel.
2023-06-26T15:04:48,798 [INFO ] main org.pytorch.serve.ModelServer - Management API bind to: http://0.0.0.0:8081
2023-06-26T15:04:48,798 [INFO ] main org.pytorch.serve.ModelServer - Management API bind to: http://0.0.0.0:8081
2023-06-26T15:04:48,798 [INFO ] main org.pytorch.serve.ModelServer - Initialize Metrics server with: EpollServerSocketChannel.
2023-06-26T15:04:48,798 [INFO ] main org.pytorch.serve.ModelServer - Initialize Metrics server with: EpollServerSocketChannel.
2023-06-26T15:04:48,799 [INFO ] main org.pytorch.serve.ModelServer - Metrics API bind to: http://0.0.0.0:8082
2023-06-26T15:04:48,799 [INFO ] main org.pytorch.serve.ModelServer - Metrics API bind to: http://0.0.0.0:8082
2023-06-26T15:04:49,136 [WARN ] pool-3-thread-1 org.pytorch.serve.metrics.MetricCollector - worker pid is not available yet.
2023-06-26T15:04:49,136 [WARN ] pool-3-thread-1 org.pytorch.serve.metrics.MetricCollector - worker pid is not available yet.
2023-06-26T15:04:49,710 [INFO ] pool-3-thread-1 TS_METRICS - CPUUtilization.Percent:0.0|#Level:Host|#hostname:torchserve-service-lm-68874fb5-x5sgq,timestamp:1687791889
2023-06-26T15:04:49,712 [INFO ] pool-3-thread-1 TS_METRICS - DiskAvailable.Gigabytes:63.784358978271484|#Level:Host|#hostname:torchserve-service-lm-68874fb5-x5sgq,timestamp:1687791889
2023-06-26T15:04:49,712 [INFO ] pool-3-thread-1 TS_METRICS - DiskUsage.Gigabytes:30.453468322753906|#Level:Host|#hostname:torchserve-service-lm-68874fb5-x5sgq,timestamp:1687791889
2023-06-26T15:04:49,713 [INFO ] pool-3-thread-1 TS_METRICS - DiskUtilization.Percent:32.3|#Level:Host|#hostname:torchserve-service-lm-68874fb5-x5sgq,timestamp:1687791889
2023-06-26T15:04:49,713 [INFO ] pool-3-thread-1 TS_METRICS - GPUMemoryUtilization.Percent:0.0|#Level:Host,DeviceId:0|#hostname:torchserve-service-lm-68874fb5-x5sgq,timestamp:1687791889
2023-06-26T15:04:49,714 [INFO ] pool-3-thread-1 TS_METRICS - GPUMemoryUsed.Megabytes:0.0|#Level:Host,DeviceId:0|#hostname:torchserve-service-lm-68874fb5-x5sgq,timestamp:1687791889
2023-06-26T15:04:49,714 [INFO ] pool-3-thread-1 TS_METRICS - GPUUtilization.Percent:0.0|#Level:Host,DeviceId:0|#hostname:torchserve-service-lm-68874fb5-x5sgq,timestamp:1687791889
2023-06-26T15:04:49,715 [INFO ] pool-3-thread-1 TS_METRICS - MemoryAvailable.Megabytes:24650.671875|#Level:Host|#hostname:torchserve-service-lm-68874fb5-x5sgq,timestamp:1687791889
2023-06-26T15:04:49,715 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUsed.Megabytes:969.5546875|#Level:Host|#hostname:torchserve-service-lm-68874fb5-x5sgq,timestamp:1687791889
2023-06-26T15:04:49,715 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUtilization.Percent:5.4|#Level:Host|#hostname:torchserve-service-lm-68874fb5-x5sgq,timestamp:1687791889
2023-06-26T15:04:50,540 [INFO ] W-9000-distilbert_1-stdout MODEL_LOG - s_name_part0=/home/model-server/tmp/.ts.sock, s_name_part1=9000, pid=271
2023-06-26T15:04:50,542 [INFO ] W-9000-distilbert_1-stdout MODEL_LOG - Listening on port: /home/model-server/tmp/.ts.sock.9000
2023-06-26T15:04:50,551 [INFO ] W-9000-distilbert_1-stdout MODEL_LOG - Successfully loaded /home/venv/lib/python3.9/site-packages/ts/configs/metrics.yaml.
2023-06-26T15:04:50,552 [INFO ] W-9000-distilbert_1-stdout MODEL_LOG - [PID]271
2023-06-26T15:04:50,553 [DEBUG] W-9000-distilbert_1 org.pytorch.serve.wlm.WorkerThread - W-9000-distilbert_1 State change null -> WORKER_STARTED
2023-06-26T15:04:50,553 [DEBUG] W-9000-distilbert_1 org.pytorch.serve.wlm.WorkerThread - W-9000-distilbert_1 State change null -> WORKER_STARTED
2023-06-26T15:04:50,553 [INFO ] W-9000-distilbert_1-stdout MODEL_LOG - Torch worker started.
2023-06-26T15:04:50,553 [INFO ] W-9000-distilbert_1-stdout MODEL_LOG - Python runtime: 3.9.16
2023-06-26T15:04:50,558 [INFO ] W-9000-distilbert_1 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9000
2023-06-26T15:04:50,558 [INFO ] W-9000-distilbert_1 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9000
2023-06-26T15:04:50,568 [INFO ] W-9000-distilbert_1-stdout MODEL_LOG - Connection accepted: /home/model-server/tmp/.ts.sock.9000.
2023-06-26T15:04:50,571 [INFO ] W-9000-distilbert_1 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD to backend at: 1687791890571
2023-06-26T15:04:50,571 [INFO ] W-9000-distilbert_1 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD to backend at: 1687791890571
2023-06-26T15:04:50,620 [INFO ] W-9000-distilbert_1-stdout MODEL_LOG - model_name: distilbert, batchSize: 1
2023-06-26T15:04:51,074 [INFO ] W-9000-distilbert_1-stdout MODEL_LOG - ONNX enabled
2023-06-26T15:04:51,074 [INFO ] W-9000-distilbert_1-stdout MODEL_LOG - {'model_dir': '/home/model-server/tmp/models/78c5f106feac4a05a70157275ba9f44f', 'gpu_id': 0, 'batch_size': 1, 'server_name': 'MMS', 'server_version': '0.8.0', 'limit_max_image_pixels': True}
2023-06-26T15:04:51,349 [WARN ] W-9000-distilbert_1-stderr MODEL_LOG - 
2023-06-26T15:04:51,358 [WARN ] W-9000-distilbert_1-stderr MODEL_LOG - Downloading (…)okenizer_config.json:   0%|          | 0.00/28.0 [00:00<?, ?B/s]
2023-06-26T15:04:51,358 [WARN ] W-9000-distilbert_1-stderr MODEL_LOG - Downloading (…)okenizer_config.json: 100%|██████████| 28.0/28.0 [00:00<00:00, 3.32kB/s]
2023-06-26T15:04:51,558 [WARN ] W-9000-distilbert_1-stderr MODEL_LOG - 
2023-06-26T15:04:51,567 [WARN ] W-9000-distilbert_1-stderr MODEL_LOG - Downloading (…)lve/main/config.json:   0%|          | 0.00/483 [00:00<?, ?B/s]
2023-06-26T15:04:51,567 [WARN ] W-9000-distilbert_1-stderr MODEL_LOG - Downloading (…)lve/main/config.json: 100%|██████████| 483/483 [00:00<00:00, 60.0kB/s]
2023-06-26T15:04:51,789 [WARN ] W-9000-distilbert_1-stderr MODEL_LOG - 
2023-06-26T15:04:51,952 [WARN ] W-9000-distilbert_1-stderr MODEL_LOG - Downloading (…)solve/main/vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]
2023-06-26T15:04:51,952 [WARN ] W-9000-distilbert_1-stderr MODEL_LOG - Downloading (…)solve/main/vocab.txt: 100%|██████████| 232k/232k [00:00<00:00, 1.42MB/s]
2023-06-26T15:04:51,953 [WARN ] W-9000-distilbert_1-stderr MODEL_LOG - Downloading (…)solve/main/vocab.txt: 100%|██████████| 232k/232k [00:00<00:00, 1.42MB/s]
2023-06-26T15:04:52,172 [WARN ] W-9000-distilbert_1-stderr MODEL_LOG - 
2023-06-26T15:04:52,179 [WARN ] W-9000-distilbert_1-stderr MODEL_LOG - Downloading (…)/main/tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]
2023-06-26T15:04:52,179 [WARN ] W-9000-distilbert_1-stderr MODEL_LOG - Downloading (…)/main/tokenizer.json: 100%|██████████| 466k/466k [00:00<00:00, 68.9MB/s]
2023-06-26T15:04:56,706 [WARN ] W-9000-distilbert_1-stderr MODEL_LOG - 2023-06-26 15:04:56.705819959 [W:onnxruntime:, inference_session.cc:1591 Initialize] Serializing optimized model with Graph Optimization level greater than ORT_ENABLE_EXTENDED and the NchwcTransformer enabled. The generated model may contain hardware specific optimizations, and should only be used in the same environment the model was optimized in.
2023-06-26T15:04:58,911 [DEBUG] W-9000-distilbert_1 org.pytorch.serve.wlm.WorkerThread - sent a reply, jobdone: true
2023-06-26T15:04:58,911 [DEBUG] W-9000-distilbert_1 org.pytorch.serve.wlm.WorkerThread - sent a reply, jobdone: true
2023-06-26T15:04:58,912 [INFO ] W-9000-distilbert_1 org.pytorch.serve.wlm.WorkerThread - Backend response time: 8292
2023-06-26T15:04:58,912 [INFO ] W-9000-distilbert_1 org.pytorch.serve.wlm.WorkerThread - Backend response time: 8292
2023-06-26T15:04:58,912 [DEBUG] W-9000-distilbert_1 org.pytorch.serve.wlm.WorkerThread - W-9000-distilbert_1 State change WORKER_STARTED -> WORKER_MODEL_LOADED
2023-06-26T15:04:58,912 [DEBUG] W-9000-distilbert_1 org.pytorch.serve.wlm.WorkerThread - W-9000-distilbert_1 State change WORKER_STARTED -> WORKER_MODEL_LOADED
2023-06-26T15:04:58,913 [INFO ] W-9000-distilbert_1 TS_METRICS - WorkerLoadTime.Milliseconds:10190.0|#WorkerName:W-9000-distilbert_1,Level:Host|#hostname:torchserve-service-lm-68874fb5-x5sgq,timestamp:1687791898
2023-06-26T15:04:58,913 [INFO ] W-9000-distilbert_1 TS_METRICS - WorkerThreadTime.Milliseconds:50.0|#Level:Host|#hostname:torchserve-service-lm-68874fb5-x5sgq,timestamp:1687791898
model-server@torchserve-service-lm-68874fb5-x5sgq:~$ 2023-06-26T15:05:30,320 [INFO ] epollEventLoopGroup-4-1 ACCESS_LOG - /10.92.5.8:44680 "GET /metrics HTTP/1.1" 200 2
2023-06-26T15:05:30,321 [INFO ] epollEventLoopGroup-4-1 TS_METRICS - Requests2XX.Count:1

model-server@torchserve-service-lm-68874fb5-x5sgq:~$ 
model-server@torchserve-service-lm-68874fb5-x5sgq:~$ 
model-server@torchserve-service-lm-68874fb5-x5sgq:~$ 
model-server@torchserve-service-lm-68874fb5-x5sgq:~$ 
model-server@torchserve-service-lm-68874fb5-x5sgq:~$ nvidia-smi 
Mon Jun 26 14:59:30 2023       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 470.161.03   Driver Version: 470.161.03   CUDA Version: 11.4     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |
| N/A   45C    P0    28W /  70W |   1414MiB / 15109MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                                  |
|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
|        ID   ID                                                   Usage      |
|=============================================================================|
+-----------------------------------------------------------------------------+
2023-06-26T14:59:30,288 [INFO ] epollEventLoopGroup-4-1 ACCESS_LOG - /10.92.5.8:45404 "GET /metrics HTTP/1.1" 200 1
2023-06-26T14:59:30,289 [INFO ] epollEventLoopGroup-4-1 TS_METRICS - Requests2XX.Count:1.0|#Level:Host|#hostname:torchserve-service-lm-68874fb5-x5sgq,timestamp:1687791570
model-server@torchserve-service-lm-68874fb5-x5sgq:~$ 
model-server@torchserve-service-lm-68874fb5-x5sgq:~$ pip list
Package                  Version
------------------------ ------------
aniso8601                9.0.1
ansi2html                1.8.0
arrow                    1.2.3
blinker                  1.6.2
captum                   0.6.0
certifi                  2023.5.7
charset-normalizer       3.1.0
click                    8.1.3
cmake                    3.26.3
coloredlogs              15.0.1
contourpy                1.0.7
cycler                   0.11.0
Cython                   0.29.34
enum-compat              0.0.3
filelock                 3.12.0
Flask                    2.3.2
Flask-RESTful            0.3.9
flatbuffers              23.5.26
fonttools                4.39.4
fsspec                   2023.6.0
huggingface-hub          0.15.1
humanfriendly            10.0
idna                     3.4
importlib-metadata       6.6.0
importlib-resources      5.12.0
itsdangerous             2.1.2
Jinja2                   3.1.2
kiwisolver               1.4.4
lit                      16.0.3
MarkupSafe               2.1.2
matplotlib               3.7.1
mpmath                   1.3.0
networkx                 3.1
numpy                    1.24.3
nvgpu                    0.10.0
nvidia-cublas-cu11       11.10.3.66
nvidia-cuda-cupti-cu11   11.7.101
nvidia-cuda-nvrtc-cu11   11.7.99
nvidia-cuda-runtime-cu11 11.7.99
nvidia-cudnn-cu11        8.5.0.96
nvidia-cufft-cu11        10.9.0.58
nvidia-curand-cu11       10.2.10.91
nvidia-cusolver-cu11     11.4.0.1
nvidia-cusparse-cu11     11.7.4.91
nvidia-nccl-cu11         2.14.3
nvidia-nvtx-cu11         11.7.91
onnx                     1.14.0
onnxruntime-gpu          1.15.1
packaging                23.1
pandas                   2.0.1
Pillow                   9.3.0
pip                      23.1.2
protobuf                 4.23.3
psutil                   5.9.5
pynvml                   11.4.1
pyparsing                3.0.9
python-dateutil          2.8.2
pytz                     2023.3
PyYAML                   6.0
regex                    2023.6.3
requests                 2.30.0
safetensors              0.3.1
setuptools               67.7.2
six                      1.16.0
sympy                    1.12
tabulate                 0.9.0
termcolor                2.3.0
tokenizers               0.13.3
torch                    2.0.0+cu117
torch-model-archiver     0.8.0
torch-workflow-archiver  0.2.8
torchaudio               2.0.1+cu117
torchdata                0.6.0
torchserve               0.8.0
torchtext                0.15.1+cpu
torchvision              0.15.1+cu117
tqdm                     4.65.0
transformers             4.30.2
triton                   2.0.0
typing_extensions        4.5.0
tzdata                   2023.3
urllib3                  2.0.2
Werkzeug                 2.3.4
wheel                    0.40.0
zipp                     3.15.0


[
  {
    "modelName": "distilbert",
    "modelVersion": "1",
    "modelUrl": "distilbert_model.mar",
    "runtime": "python",
    "minWorkers": 1,
    "maxWorkers": 1,
    "batchSize": 1,
    "maxBatchDelay": 100,
    "loadedAtStartup": true,
    "workers": [
      {
        "id": "9000",
        "startTime": "2023-06-26T14:59:11.731Z",
        "status": "READY",
        "memoryUsage": 3392032768,
        "pid": 186,
        "gpu": true,
        "gpuUsage": "gpuId::0 utilization.gpu [%]::0 % utilization.memory [%]::0 % memory.used [MiB]::1414 MiB"
      }
    ]
  }
]
